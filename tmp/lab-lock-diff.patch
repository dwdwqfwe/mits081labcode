diff --git a/kernel/bio.c b/kernel/bio.c
index 60d91a6..2d6043d 100644
--- a/kernel/bio.c
+++ b/kernel/bio.c
@@ -22,33 +22,41 @@
 #include "defs.h"
 #include "fs.h"
 #include "buf.h"
-
+#define NBUCKT 13
+#define hash(x) (x)%NBUCKT
+// int
+// hash(int blockno)
+// {
+//   return blockno % NBUCKT;
+// }
 struct {
-  struct spinlock lock;
+  struct spinlock lock[NBUCKT];
   struct buf buf[NBUF];
-
+  struct spinlock biglock;
   // Linked list of all buffers, through prev/next.
   // Sorted by how recently the buffer was used.
   // head.next is most recent, head.prev is least.
-  struct buf head;
+  struct buf head[NBUCKT];
 } bcache;
-
 void
 binit(void)
 {
   struct buf *b;
-
-  initlock(&bcache.lock, "bcache");
+  for(int i=0;i<NBUCKT;i++){
+  initlock(&bcache.lock[i], "bcache");
+  }
 
   // Create linked list of buffers
-  bcache.head.prev = &bcache.head;
-  bcache.head.next = &bcache.head;
+  for(int i=0;i<NBUCKT;i++){
+  bcache.head[i].prev = &bcache.head[i];
+  bcache.head[i].next = &bcache.head[i];
+  }
   for(b = bcache.buf; b < bcache.buf+NBUF; b++){
-    b->next = bcache.head.next;
-    b->prev = &bcache.head;
+    b->next = bcache.head[0].next;
+    b->prev = &bcache.head[0];
     initsleeplock(&b->lock, "buffer");
-    bcache.head.next->prev = b;
-    bcache.head.next = b;
+    bcache.head[0].next->prev = b;
+    bcache.head[0].next = b;
   }
 }
 
@@ -59,35 +67,182 @@ static struct buf*
 bget(uint dev, uint blockno)
 {
   struct buf *b;
-
-  acquire(&bcache.lock);
+  struct buf *b2=0;
+  int minsize=0;
+  uint64 id=hash(blockno);
+  acquire(&bcache.lock[id]);
 
   // Is the block already cached?
-  for(b = bcache.head.next; b != &bcache.head; b = b->next){
+  for(b = bcache.head[id].next; b != &bcache.head[id]; b = b->next){
     if(b->dev == dev && b->blockno == blockno){
       b->refcnt++;
-      release(&bcache.lock);
+      release(&bcache.lock[id]);
       acquiresleep(&b->lock);
       return b;
     }
   }
-
-  // Not cached.
-  // Recycle the least recently used (LRU) unused buffer.
-  for(b = bcache.head.prev; b != &bcache.head; b = b->prev){
-    if(b->refcnt == 0) {
-      b->dev = dev;
-      b->blockno = blockno;
-      b->valid = 0;
-      b->refcnt = 1;
-      release(&bcache.lock);
+  release(&bcache.lock[id]);
+  acquire(&bcache.biglock);
+  acquire(&bcache.lock[id]);
+    for (b = bcache.head[id].next; b != &bcache.head[id]; b = b->next) {
+    if(b->dev == dev && b->blockno == blockno) {
+      b->refcnt++;
+      release(&bcache.lock[id]);
+      release(&bcache.biglock);
       acquiresleep(&b->lock);
       return b;
     }
   }
+  // Not cached.
+  // Recycle the least recently used (LRU) unused buffer.
+  for(b = bcache.head[id].next; b != &bcache.head[id]; b = b->next){
+    if(b->refcnt == 0&&(b->lastuser<minsize||b2==0)) {
+      minsize=b->lastuser;
+      b2=b;
+    }
+  }
+  if(b2){
+      b2->dev = dev;
+      b2->blockno = blockno;
+      b2->valid = 0;
+      b2->refcnt = 1;
+      // b2->lastuser=ticks;
+      release(&bcache.lock[id]);
+      release(&bcache.biglock);
+      acquiresleep(&b2->lock);
+    return b2;
+  }
+  for(int i=hash(id+1);i!=id;i=(hash(i+1)))
+  {
+    acquire(&bcache.lock[i]);
+    for(b = bcache.head[i].next; b != &bcache.head[i]; b = b->next){
+    if(b->refcnt == 0&&(b->lastuser<minsize||b2==0)) {
+      minsize=b->lastuser;
+      b2=b;
+    }
+  }
+  if(b2)
+  {
+    b2->dev = dev;
+    b2->blockno = blockno;
+    b2->valid = 0;
+    b2->refcnt = 1;
+    // b2->lastuser=ticks;
+    //  b2->next->prev = b2->prev;
+    //   b2->prev->next = b2->next;
+    //   b2->next = bcache.head[id].next;
+    //   b2->prev = &bcache.head[id];
+    //   bcache.head[id].next->prev = b2;
+    //   bcache.head[id].next = b2;
+    //   release(&bcache.lock[i]);
+    //   release(&bcache.lock[id]);
+    //   release(&bcache.biglock);
+    //   acquiresleep(&b2->lock);
+    b2->next->prev = b2->prev;
+      b2->prev->next = b2->next;
+      release(&bcache.lock[i]);
+      // add block
+      b2->next = bcache.head[id].next;
+      b2->prev = &bcache.head[id];
+      bcache.head[id].next->prev = b2;
+      bcache.head[id].next = b2;
+      release(&bcache.lock[id]);
+      release(&bcache.biglock);
+      acquiresleep(&b2->lock);
+    return b2;
+  }
+  release(&bcache.lock[i]);
+  }
+   release(&bcache.lock[id]);
+   release(&bcache.biglock);
   panic("bget: no buffers");
 }
 
+// static struct buf*
+// bget(uint dev, uint blockno)
+// {
+//   struct buf *b, *b2 = 0;
+
+//   int i = hash(blockno), min_ticks = 0;
+//   acquire(&bcache.lock[i]);
+
+//   // 1. Is the block already cached?
+//   for(b = bcache.head[i].next; b != &bcache.head[i]; b = b->next){
+//     if(b->dev == dev && b->blockno == blockno){
+//       b->refcnt++;
+//       release(&bcache.lock[i]);
+//       acquiresleep(&b->lock);
+//       return b;
+//     }
+//   }
+//   release(&bcache.lock[i]);
+
+//   // 2. Not cached.
+//   acquire(&bcache.biglock);
+//   acquire(&bcache.lock[i]);
+//   // 2.1 find from current bucket.
+//   for (b = bcache.head[i].next; b != &bcache.head[i]; b = b->next) {
+//     if(b->dev == dev && b->blockno == blockno) {
+//       b->refcnt++;
+//       release(&bcache.lock[i]);
+//       release(&bcache.biglock);
+//       acquiresleep(&b->lock);
+//       return b;
+//     }
+//   }
+//   // 2.2 find a LRU block from current bucket.
+//   for (b = bcache.head[i].next; b != &bcache.head[i]; b = b->next) {
+//     if (b->refcnt == 0 && (b2 == 0 || b->lastuser < min_ticks)) {
+//       min_ticks = b->lastuser;
+//       b2 = b;
+//     }
+//   }
+//   if (b2) {
+//     b2->dev = dev;
+//     b2->blockno = blockno;
+//     b2->refcnt++;
+//     b2->valid = 0;
+//     //acquiresleep(&b2->lock);
+//     release(&bcache.lock[i]);
+//     release(&bcache.biglock);
+//     acquiresleep(&b2->lock);
+//     return b2;
+//   }
+//   // 2.3 find block from the other buckets.
+//   for (int j = hash(i + 1); j != i; j = hash(j + 1)) {
+//     acquire(&bcache.lock[j]);
+//     for (b = bcache.head[j].next; b != &bcache.head[j]; b = b->next) {
+//       if (b->refcnt == 0 && (b2 == 0 || b->lastuser < min_ticks)) {
+//         min_ticks = b->lastuser;
+//         b2 = b;
+//       }
+//     }
+//     if(b2) {
+//       b2->dev = dev;
+//       b2->refcnt++;
+//       b2->valid = 0;
+//       b2->blockno = blockno;
+//       // remove block from its original bucket.
+//       b2->next->prev = b2->prev;
+//       b2->prev->next = b2->next;
+//       release(&bcache.lock[j]);
+//       // add block
+//       b2->next = bcache.head[i].next;
+//       b2->prev = &bcache.head[i];
+//       bcache.head[i].next->prev = b2;
+//       bcache.head[i].next = b2;
+//       release(&bcache.lock[i]);
+//       release(&bcache.biglock);
+//       acquiresleep(&b2->lock);
+//       return b2;
+//     }
+//     release(&bcache.lock[j]);
+//   }
+//   release(&bcache.lock[i]);
+//   release(&bcache.biglock);
+//   panic("bget: no buffers");
+// }
+
 // Return a locked buf with the contents of the indicated block.
 struct buf*
 bread(uint dev, uint blockno)
@@ -99,6 +254,7 @@ bread(uint dev, uint blockno)
     virtio_disk_rw(b, 0);
     b->valid = 1;
   }
+  // printf("%d\n",b->dev);
   return b;
 }
 
@@ -120,34 +276,38 @@ brelse(struct buf *b)
     panic("brelse");
 
   releasesleep(&b->lock);
-
-  acquire(&bcache.lock);
+  int id=hash(b->blockno);
+  acquire(&bcache.lock[id]);
   b->refcnt--;
   if (b->refcnt == 0) {
     // no one is waiting for it.
-    b->next->prev = b->prev;
-    b->prev->next = b->next;
-    b->next = bcache.head.next;
-    b->prev = &bcache.head;
-    bcache.head.next->prev = b;
-    bcache.head.next = b;
+    // b->next->prev = b->prev;
+    // b->prev->next = b->next;
+    // b->next = bcache.head[id].next;
+    // b->prev = &bcache.head[id];
+    // bcache.head[id].next->prev = b;
+    // bcache.head[id].next = b;
+    b->lastuser=ticks;
   }
   
-  release(&bcache.lock);
+  release(&bcache.lock[id]);
 }
 
+
 void
 bpin(struct buf *b) {
-  acquire(&bcache.lock);
+  int id=hash(b->blockno);
+  acquire(&bcache.lock[id]);
   b->refcnt++;
-  release(&bcache.lock);
+  release(&bcache.lock[id]);
 }
 
 void
 bunpin(struct buf *b) {
-  acquire(&bcache.lock);
+  int id=hash(b->blockno);
+  acquire(&bcache.lock[id]);
   b->refcnt--;
-  release(&bcache.lock);
+  release(&bcache.lock[id]);
 }
 
 
diff --git a/kernel/buf.h b/kernel/buf.h
index 4616e9e..f0cba7e 100644
--- a/kernel/buf.h
+++ b/kernel/buf.h
@@ -8,5 +8,6 @@ struct buf {
   struct buf *prev; // LRU cache list
   struct buf *next;
   uchar data[BSIZE];
+  uint lastuser;
 };
 
diff --git a/kernel/fs.c b/kernel/fs.c
index 848b2c9..6d66b4f 100644
--- a/kernel/fs.c
+++ b/kernel/fs.c
@@ -298,6 +298,7 @@ ilock(struct inode *ip)
 
   if(ip->valid == 0){
     bp = bread(ip->dev, IBLOCK(ip->inum, sb));
+    // printf("sss\n");
     dip = (struct dinode*)bp->data + ip->inum%IPB;
     ip->type = dip->type;
     ip->major = dip->major;
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index fa6a0ac..db137e9 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -9,6 +9,7 @@
 #include "riscv.h"
 #include "defs.h"
 
+
 void freerange(void *pa_start, void *pa_end);
 
 extern char end[]; // first address after kernel.
@@ -21,12 +22,18 @@ struct run {
 struct {
   struct spinlock lock;
   struct run *freelist;
-} kmem;
-
+} kmem[NCPU];
+struct
+{
+  struct spinlock lock;
+  struct run *freelist;
+}kmem_cpu[NCPU];
 void
 kinit()
 {
-  initlock(&kmem.lock, "kmem");
+  for(int i=0;i<NCPU;i++){
+  initlock(&kmem[i].lock, "kmem");
+  }
   freerange(end, (void*)PHYSTOP);
 }
 
@@ -34,7 +41,10 @@ void
 freerange(void *pa_start, void *pa_end)
 {
   char *p;
-  p = (char*)PGROUNDUP((uint64)pa_start);
+  // uint64 n=cpuid();
+  // printf("%d\n",n);
+  // uint64 nsize=(uint64)(pa_end-pa_start)/NCPU;
+  p = (char*)(PGROUNDUP((uint64)pa_start));
   for(; p + PGSIZE <= (char*)pa_end; p += PGSIZE)
     kfree(p);
 }
@@ -53,13 +63,13 @@ kfree(void *pa)
 
   // Fill with junk to catch dangling refs.
   memset(pa, 1, PGSIZE);
-
+  uint64 n=cpuid();
   r = (struct run*)pa;
 
-  acquire(&kmem.lock);
-  r->next = kmem.freelist;
-  kmem.freelist = r;
-  release(&kmem.lock);
+  acquire(&kmem[n].lock);
+  r->next = kmem[n].freelist;
+  kmem[n].freelist = r;
+  release(&kmem[n].lock);
 }
 
 // Allocate one 4096-byte page of physical memory.
@@ -69,12 +79,29 @@ void *
 kalloc(void)
 {
   struct run *r;
+  int n=cpuid();
+  acquire(&kmem[n].lock);
+  r = kmem[n].freelist;
+  if(!r){
+        for(int i=0;i<NCPU;i++)
+    {
+      if(i==n){
+      continue;
+      }
 
-  acquire(&kmem.lock);
-  r = kmem.freelist;
-  if(r)
-    kmem.freelist = r->next;
-  release(&kmem.lock);
+      if(kmem[i].freelist)
+      {
+        kmem[n].freelist=kmem[i].freelist;
+        r=kmem[n].freelist;
+        kmem[i].freelist=(struct run*)(0);
+        break;
+      }
+    }
+  }
+  if(r){
+    kmem[n].freelist = r->next;
+  }
+  release(&kmem[n].lock);
 
   if(r)
     memset((char*)r, 5, PGSIZE); // fill with junk
diff --git a/time.txt b/time.txt
new file mode 100644
index 0000000..3b8dc09
--- /dev/null
+++ b/time.txt
@@ -0,0 +1 @@
+55555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555
